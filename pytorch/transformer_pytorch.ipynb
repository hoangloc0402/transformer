{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "3dim silly_transformer_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ49R389Ixxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/My Drive/Colab Notebooks/\"\n",
        "\n",
        "with open(path + 'tokenizer_en.pickle', 'rb') as f:\n",
        "    tokenizer_en = pickle.load(f)\n",
        "with open(path + 'tokenizer_pt.pickle', 'rb') as f:\n",
        "    tokenizer_pt = pickle.load(f)\n",
        "with open(path + 'tokenized_data.pickle', 'rb') as f:\n",
        "    tokenized_data = pickle.load(f)\n",
        "\n",
        "data = \"train\"\n",
        "train_X = tokenized_data[\"pt.\" + data]\n",
        "train_y_inp = tokenized_data[\"en.\" + data + \".inp\"]\n",
        "train_y_tar = tokenized_data[\"en.\" + data + \".tar\"]\n",
        "\n",
        "train_X = train_X.reshape(train_X.shape[0]//40, 40)\n",
        "train_y_inp = train_y_inp.reshape(train_y_inp.shape[0]//40, 40)\n",
        "train_y_tar = train_y_tar.reshape(train_y_tar.shape[0]//40, 40)\n",
        "\n",
        "train_X = torch.from_numpy(train_X)\n",
        "train_y_inp = torch.from_numpy(train_y_inp)\n",
        "train_y_tar = torch.from_numpy(train_y_tar)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fxrA6uDH64q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        depth = d_model // num_heads\n",
        "        self.dense_q, self.dense_k, self.dense_v = list(), list(), list()\n",
        "        \n",
        "        for layer in [self.dense_q, self.dense_k, self.dense_v]:\n",
        "            for head_id in range(num_heads):\n",
        "                layer.append(nn.Linear(d_model, depth).to(device))\n",
        "        self.dense_final = nn.Linear(d_model, d_model).to(device)\n",
        "    \n",
        "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
        "        matmul_qk = torch.matmul(q, k.permute(0,2,1)) #(B*SQ, SK)\n",
        "        sqrt_dk = torch.Tensor([np.sqrt(40)]).to(device) #Should be sequence length\n",
        "        scaled_attention_logits = matmul_qk / sqrt_dk\n",
        "        # print(scaled_attention_logits.shape, padding_mask.shape)\n",
        "        scaled_attention_logits += mask \n",
        "        scaled_attention = nn.Softmax(2)(scaled_attention_logits)\n",
        "        output =  torch.matmul(scaled_attention, v) #(B*SQ, DEPTH)\n",
        "        return output\n",
        "    \n",
        "    def forward(self, q, k, v, mask):\n",
        "        # assert k.shape[0].value == v.shape[0].value\n",
        "        head_summaries = list()\n",
        "        for head_id in range(self.num_heads):\n",
        "            out_q = self.dense_q[head_id](q) # (B*SQ, DEPTH)\n",
        "            out_k = self.dense_k[head_id](k) # (B*SK, DEPTH)\n",
        "            out_v = self.dense_v[head_id](v) # (B*SK, DEPTH)     \n",
        "            head_attention_weights = self.scaled_dot_product_attention(out_q, out_k, out_v, mask)\n",
        "            head_summaries.append(head_attention_weights) \n",
        "        \n",
        "        concat_attention = torch.cat(head_summaries, axis=2).to(device) # (B*SQ, D_MODEL)\n",
        "        output = self.dense_final(concat_attention) # (B*SQ, D_MODEL)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0QH9i95Vmlb",
        "colab_type": "text"
      },
      "source": [
        "# CELL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KetgWREBH64s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MHA_Cell(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout_rate):\n",
        "        super(MHA_Cell, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.dropout = nn.Dropout(dropout_rate).to(device)\n",
        "        self.layernorm = nn.LayerNorm(d_model).to(device)\n",
        "        \n",
        "    def forward(self, q, k, v, mask):\n",
        "        output = self.mha(q, k, v, mask)  # (B*SQ, D_MODEL)\n",
        "        output = self.dropout(output)\n",
        "        output = self.layernorm(q + output)\n",
        "        return output\n",
        "\n",
        "    \n",
        "class FFN_Cell(nn.Module):\n",
        "    def __init__(self, d_model, dff, dropout_rate):\n",
        "        super(FFN_Cell, self).__init__()\n",
        "        self.sequential = nn.Sequential(\n",
        "            nn.Linear(d_model, dff).to(device), # (B*SQ, DFF)\n",
        "            nn.ReLU().to(device),\n",
        "            nn.Linear(dff, d_model).to(device)  # (B*SQ, D_MODEL)\n",
        "        ).to(device)\n",
        "        self.dropout = nn.Dropout(dropout_rate).to(device)\n",
        "        self.layernorm = nn.LayerNorm(d_model).to(device)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.sequential(x)\n",
        "        output = self.dropout(output)\n",
        "        output = self.layernorm(x + output)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrghzEXhVqC-",
        "colab_type": "text"
      },
      "source": [
        "# LAYER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBnuvc72H64v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha_cell = MHA_Cell(d_model, num_heads, dropout_rate)\n",
        "        self.ffn_cell = FFN_Cell(d_model, dff, dropout_rate)\n",
        "    \n",
        "    def forward(self, x, mask):\n",
        "        # print(\"    ENCODER MHA\")\n",
        "        output = self.mha_cell(x, x, x, mask)\n",
        "        # print(\"    ENCODER FFN\")\n",
        "        output = self.ffn_cell(output)\n",
        "        return output\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.mha_cell_1 = MHA_Cell(d_model, num_heads, dropout_rate)\n",
        "        self.mha_cell_2 = MHA_Cell(d_model, num_heads, dropout_rate)\n",
        "        self.ffn_cell = FFN_Cell(d_model, dff, dropout_rate)\n",
        "        \n",
        "    def forward(self, x, encoder_output, input_padding_mask, look_ahead_mask):\n",
        "        # print(\"    DECODER MHA 1\")\n",
        "        output = self.mha_cell_1(x, x, x, look_ahead_mask)\n",
        "        # print(\"    DECODER MHA 2\")\n",
        "        output = self.mha_cell_2(output, encoder_output, encoder_output, input_padding_mask)      \n",
        "        # print(\"    DECODER FFN\")\n",
        "        output = self.ffn_cell(output)   \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIdE8wuCH64y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, dropout_rate):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout_rate).to(device)\n",
        "        self.encoder_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        # print(\"  ENCODER: \")\n",
        "        x = self.dropout(x)\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            x = encoder_layer(x, mask)\n",
        "        # print(\"  ENCODER FINISHED!\")\n",
        "        return x  # (B*S, D_MODEL)\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, dropout_rate):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout_rate).to(device)\n",
        "        self.decoder_layers = [DecoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
        "        \n",
        "    def forward(self, x, encoder_output, input_padding_mask, look_ahead_mask):\n",
        "        # print(\"  DECODER: \")\n",
        "        x = self.dropout(x)\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            x = decoder_layer(x, encoder_output, input_padding_mask, look_ahead_mask)\n",
        "        # print(\"  DECODER FINISHED!\")\n",
        "        return x # (B*S, D_MODEL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUCxqZewVs7o",
        "colab_type": "text"
      },
      "source": [
        "# TRANSFORMER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ktHxYupH641",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, \n",
        "                num_layers, d_model, num_heads, dff, \n",
        "                input_vocab_size, target_vocab_size, \n",
        "                pos_enc_size, dropout_rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.input_embedding = nn.Embedding(input_vocab_size, d_model)\n",
        "        self.target_embedding = nn.Embedding(target_vocab_size, d_model)\n",
        "        \n",
        "        self.pos_enc = self.get_position_encoding_matrix(pos_enc_size, d_model) #(B, D_MODEL)\n",
        "        self.pos_enc_tiled = self.pos_enc \n",
        "        \n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, dropout_rate)  \n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, dropout_rate)\n",
        "        \n",
        "        self.linear = nn.Linear(d_model, target_vocab_size).to(device)\n",
        "        self.softmax = nn.Softmax(2).to(device)\n",
        "    \n",
        "    def forward(self, \n",
        "                inputs, #(B*S, 1)\n",
        "                targets, #(B*S, 1)\n",
        "                batch_size=1):\n",
        "        # print(\"TRANSFORMER FORWARD: \")\n",
        "        input_padding_mask, look_ahead_mask = self.create_masks(inputs, targets)\n",
        "        sqrt_d_model = torch.Tensor([np.sqrt(self.d_model)]).to(device)\n",
        "        inputs  = self.input_embedding(inputs) *  sqrt_d_model #(B*S, D_MODEL)\n",
        "        targets = self.target_embedding(targets) * sqrt_d_model #(B*S, D_MODEL)\n",
        "        inputs  += self.pos_enc\n",
        "        targets += self.pos_enc\n",
        "        encoder_output = self.encoder(inputs, input_padding_mask)  # (B*S, D_MODEL)\n",
        "        decoder_output = self.decoder(targets, encoder_output, input_padding_mask, look_ahead_mask) \n",
        "        # (B*S, D_MODEL)\n",
        "\n",
        "        output = self.linear(decoder_output)  # (B*S, target_vocab_size)\n",
        "        # output = self.softmax(output) # (B*S, target_vocab_size)\n",
        "        # print(\"TRANSFORMER FINISHED!\\n\")\n",
        "        return output\n",
        "    \n",
        "    def get_position_encoding_matrix(self, num_position, d_model, min_rate = 1/10000):\n",
        "        angle_rates = min_rate**(np.linspace(0, 1, d_model//2))\n",
        "        positions = np.arange(num_position) \n",
        "        angle_rads = (positions[:, np.newaxis]) * angle_rates[np.newaxis, :]\n",
        "        sines, cosines = np.sin(angle_rads), np.cos(angle_rads)\n",
        "        pos_encoding = np.stack([sines, cosines], axis=2).reshape(sines.shape[0], -1)\n",
        "        pos_encoding = np.squeeze(pos_encoding)\n",
        "        pos_encoding = torch.from_numpy(pos_encoding).to(device)\n",
        "        return pos_encoding\n",
        "\n",
        "    def create_masks(self, inputs, targets):\n",
        "        def create_padding_mask(seq):\n",
        "            mapping_func = np.vectorize(lambda x: x == 0, otypes=[np.float32])\n",
        "            mask = mapping_func(np.copy(seq.cpu().numpy()))\n",
        "            mask = mask[:, np.newaxis, :]\n",
        "            return mask\n",
        "\n",
        "        def create_look_ahead_mask(size):\n",
        "            ones = np.ones((size, size), dtype=np.float32)\n",
        "            look_ahead_mask = np.triu(ones, k=1)\n",
        "            return look_ahead_mask[np.newaxis, :, :]\n",
        "        \n",
        "        input_padding_mask = create_padding_mask(inputs)\n",
        "        target_padding_mask = create_padding_mask(targets)\n",
        "        look_ahead_mask = create_look_ahead_mask(targets.shape[1])\n",
        "        look_ahead_mask = np.maximum(look_ahead_mask, target_padding_mask)\n",
        "    \n",
        "        input_padding_mask *= -1e9\n",
        "        look_ahead_mask *= -1e9\n",
        "        \n",
        "        input_padding_mask = torch.from_numpy(input_padding_mask).to(device)\n",
        "        look_ahead_mask = torch.from_numpy(look_ahead_mask).to(device)\n",
        "\n",
        "        return input_padding_mask, look_ahead_mask         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRCUiBUwH646",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerModel():\n",
        "    def __init__(self):\n",
        "        self.MAX_SEQ_LEN = 40\n",
        "        self.BATCH_SIZE = 64\n",
        "        self.model = Transformer(num_layers = 4,\n",
        "                                 d_model = 128,\n",
        "                                 num_heads = 8,\n",
        "                                 dff = 256, \n",
        "                                 input_vocab_size = tokenizer_pt.vocab_size + 2,\n",
        "                                 target_vocab_size = tokenizer_en.vocab_size + 2,\n",
        "                                 pos_enc_size = self.MAX_SEQ_LEN,\n",
        "                                 dropout_rate = 0.1)\n",
        "        self.model = self.model.to(device)\n",
        "        self.loss = nn.CrossEntropyLoss().to(device)\n",
        "        self.opt = torch.optim.SGD(self.model.parameters(), lr=10)\n",
        "        # self.opt = optim.Adam(self.model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-06)\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.opt, 1.0, gamma=0.95)\n",
        "        \n",
        "    def fit(self, x, y_inp, y_tar, epochs = 1):\n",
        "        num_batch = int(x.shape[0]//self.BATCH_SIZE)\n",
        "        self.model.train()\n",
        "        for epoch in range(epochs):\n",
        "          print(\"EPOCH: \", epoch)\n",
        "          accu, count = 0, 0\n",
        "          for i in range(num_batch):\n",
        "            begin = i*self.BATCH_SIZE\n",
        "            end = begin + self.BATCH_SIZE\n",
        "            x_i = x[begin:end].to(device)\n",
        "            y_i = y_inp[begin:end].to(device)\n",
        "            t_i = y_tar[begin:end].to(device)\n",
        "\n",
        "            self.opt.zero_grad()\n",
        "            prediction = self.model(x_i, y_i)\n",
        "            prediction = prediction.view(prediction.shape[0]*prediction.shape[1], prediction.shape[2])\n",
        "            t_i = t_i.flatten()\n",
        "      \n",
        "            l = self.loss(prediction, t_i)\n",
        "            l.backward()\n",
        "            accu += l.item()\n",
        "            count +=1\n",
        "            if i%100==0:\n",
        "              print(\"    Loss: \", accu/count)\n",
        "              accu, count = 0, 0\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)\n",
        "            self.opt.step()\n",
        "        \n",
        "    def get_model(self):\n",
        "        return self.model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6JJRICsxZjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 14"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahUJ3skIH648",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transformer = TransformerModel()\n",
        "for i in range(30):\n",
        "  epochs = 20\n",
        "  transformer.fit(train_X, train_y_inp, train_y_tar, epochs)\n",
        "  model_save_name = F\"transformer_{count}.pt\"\n",
        "  path = F\"/content/drive/My Drive/Save_model/{model_save_name}\" \n",
        "  with open(path, 'wb') as f:\n",
        "      pickle.dump(transformer.model, f, pickle.HIGHEST_PROTOCOL)\n",
        "  count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THqNIgbatX-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_save_name = 'transformer_0.pt'\n",
        "path = F\"/content/drive/My Drive/Save_model/{model_save_name}\" \n",
        "torch.save(transformer.model.state_dict(), path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYZe479AH65N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_token_en = [tokenizer_en.vocab_size]\n",
        "end_token_en = [tokenizer_en.vocab_size + 1]\n",
        "start_token_pt = [tokenizer_pt.vocab_size]\n",
        "end_token_pt = [tokenizer_pt.vocab_size + 1]\n",
        "pred = list()\n",
        "MAX_SEQ_LEN = 40\n",
        "\n",
        "def translate(model, sentence):\n",
        "    encoder_input = [start_token_pt] + [[i] for i in tokenizer_pt.encode(sentence)] + [end_token_pt]\n",
        "    encoder_input = encoder_input + [[0]] * (MAX_SEQ_LEN - len(encoder_input))\n",
        "    encoder_input = torch.from_numpy(np.asarray(encoder_input))\n",
        "    encoder_input = torch.squeeze(encoder_input)\n",
        "    encoder_input = torch.unsqueeze(encoder_input, 0).to(device)\n",
        "    decoder_input = [start_token_en]\n",
        "    model.eval()\n",
        "    for i in range(MAX_SEQ_LEN):\n",
        "      with torch.no_grad():\n",
        "          output = decoder_input + [[0]] * (MAX_SEQ_LEN - len(decoder_input))\n",
        "          output = torch.squeeze(torch.from_numpy(np.asarray(output))).to(device)\n",
        "          output = torch.unsqueeze(output, 0)\n",
        "          # print(\"TRANSLATE INPUT SHAPE \", encoder_input.shape, output.shape)\n",
        "          prediction = model(inputs=encoder_input, targets=output)\n",
        "          prediction = nn.Softmax(2)(prediction)\n",
        "          prediction = prediction.view(prediction.shape[0]*prediction.shape[1], prediction.shape[2])\n",
        "          # print(\"PREDICTION: \", prediction.shape)\n",
        "          last_word = prediction[len(decoder_input)-1,:].cpu().numpy()\n",
        "          pred.append(prediction)\n",
        "          predicted_id = np.argmax(last_word)\n",
        "          if predicted_id == end_token_en:\n",
        "              break\n",
        "          decoder_input.append([predicted_id])\n",
        "\n",
        "    print(decoder_input)\n",
        "    token_list = [int(token[0]) for token in decoder_input if token[0] < tokenizer_en.vocab_size]\n",
        "    translated_sentence = tokenizer_en.decode(token_list)\n",
        "    return translated_sentence    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z90tsQDZH65P",
        "colab_type": "code",
        "outputId": "4b08d2f9-1da0-415a-a645-a0ce3647bd36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "seq = \"este é um problema que temos que resolver.\" \n",
        "translate(transformer.get_model(), seq)\n",
        "#this is a problem we have to solve"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8087], [18], [14], [24], [7], [328], [6], [385], [1], [14], [24], [5], [966], [385], [2]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'so we have a problem of problem , we have to solve problem .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcOAPkZGUjrA",
        "colab_type": "code",
        "outputId": "0a51c872-4471-4d00-e21a-a9be4e27a3ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "seq = \"os meus vizinhos ouviram sobre esta ideia.\" \n",
        "translate(transformer.get_model(), seq)\n",
        "#and my neighboring homes heard about this idea ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8087], [12], [198], [126], [147], [10], [12], [98], [7941], [7870], [26], [101], [3], [1137], [10], [12], [1329], [5093], [28], [32], [626], [2]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i started two things that i did n't know the word that i totally obsessed with my idea .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIlgVdXiUjtm",
        "colab_type": "code",
        "outputId": "34d408a9-5737-4f07-9eab-be99c9e4aafe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "seq = \"este é o primeiro livro que eu fiz.\" \n",
        "translate(transformer.get_model(), seq)\n",
        "#this is the first book i've ever done."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8087], [12], [1228], [16], [774], [13], [124], [774], [13], [3], [124], [6], [16], [774], [1401], [60], [3], [124], [774], [13], [657], [2]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i wrote this book is first book is the first of this book written by the first book is wrong .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpmEAY8lJCxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}