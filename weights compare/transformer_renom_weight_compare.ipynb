{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import renom as rm\n",
    "import renom.graph as rmg\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "from tensorboardX import SummaryWriter\n",
    "if not rm.has_cuda():\n",
    "    print(\"NO CUDA!!!!!\")\n",
    "rm.set_cuda_active(rm.has_cuda())\n",
    "rm.set_cuda_active(False)\n",
    "tokenized_data, tokenizer_en, tokenizer_pt = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(rmg.core.GraphFactory):\n",
    "    def prepare(self, d_model, num_heads):\n",
    "        assert d_model % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        depth = d_model // num_heads\n",
    "        self.dense_q, self.dense_k, self.dense_v = list(), list(), list()\n",
    "        \n",
    "        for layer in [self.dense_q, self.dense_k, self.dense_v]:\n",
    "            for head_id in range(num_heads):\n",
    "                dense = rmg.Dense(depth)\n",
    "                layer.append(dense)\n",
    "        self.dense_final = rmg.Dense(d_model)\n",
    "        \n",
    "    def custom_matmul(self, a, b, max_seq_len = 40, transpose_b = True):\n",
    "        if int(a.shape[0]) == max_seq_len:\n",
    "            if transpose_b:\n",
    "                return a@b.T\n",
    "            else:\n",
    "                return a@b\n",
    "        mul = list()\n",
    "        for i in range(int(a.shape[0])//max_seq_len):\n",
    "            begin = i * max_seq_len\n",
    "            end = begin + max_seq_len\n",
    "            a_i = a[begin:end]\n",
    "            b_i = b[begin:end]\n",
    "            if transpose_b:\n",
    "                b_i = b_i.T\n",
    "            ab = a_i @ b_i\n",
    "            mul.append(ab)\n",
    "        result = rmg.concatenate(mul)\n",
    "        return result\n",
    "    \n",
    "    def scaled_dot_product_attention(self, \n",
    "                                     q, # (B*SQ, DEPTH)\n",
    "                                     k, # (B*SK, DEPTH)\n",
    "                                     v, # (B*SK, DEPTH)\n",
    "                                     mask):\n",
    "        matmul_qk = self.custom_matmul(q, k) #(B*SQ, SK)\n",
    "#         dk = float(q.shape[0]) #Should be sequence length\n",
    "        scaled_attention_logits = matmul_qk / np.sqrt(40)\n",
    "        scaled_attention_logits += mask \n",
    "        scaled_attention = rmg.softmax(scaled_attention_logits)\n",
    "        output =  self.custom_matmul(scaled_attention, v, transpose_b = False) #(B*SQ, DEPTH)\n",
    "        return output\n",
    "    \n",
    "    def connect(self, \n",
    "                q, # (B*SQ, D_MODEL)\n",
    "                k, # (B*SK, D_MODEL)\n",
    "                v, # (B*SK, D_MODEL)\n",
    "                mask):\n",
    "#         assert k.shape[0].value == v.shape[0].value\n",
    "        head_summaries = list()\n",
    "        \n",
    "        for head_id in range(self.num_heads):\n",
    "            out_q = self.dense_q[head_id](q) # (B*SQ, DEPTH)\n",
    "            out_k = self.dense_k[head_id](k) # (B*SK, DEPTH)\n",
    "            out_v = self.dense_v[head_id](v) # (B*SK, DEPTH)     \n",
    "            head_attention_weights = self.scaled_dot_product_attention(out_q, out_k, out_v, mask)\n",
    "            head_summaries.append(head_attention_weights) \n",
    "#         print(\"CONCAT\")\n",
    "        if self.num_heads == 1:\n",
    "            concat_attention = head_summaries[0]\n",
    "        else:\n",
    "            concat_attention = rmg.concatenate(head_summaries, axis=1) # (B*SQ, D_MODEL)\n",
    "#         print(\"CONCAT_END\")\n",
    "        output = self.dense_final(concat_attention) # (B*SQ, D_MODEL)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(32, 4)\n",
    "q = np.arange(0, 2*40*32).reshape(2*40, 32)\n",
    "k = np.arange(0 + 10, 2*40*32 + 10).reshape(2*40, 32)\n",
    "v = np.arange(0 + 20, 2*40*32 + 20).reshape(2*40, 32)\n",
    "mask = np.arange(2*40*40).reshape(2*40, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in [mha.dense_q, mha.dense_k, mha.dense_v]:\n",
    "    for dense in layer:\n",
    "        weight = rmg.DynamicVariable(np.arange(32*8).reshape(32, 8))\n",
    "        bias = rmg.DynamicVariable(np.arange(8).reshape(1,8))\n",
    "        dense.params[\"w\"] = weight\n",
    "        dense.params[\"b\"] = bias\n",
    "        \n",
    "weight = rmg.DynamicVariable(np.arange(32*32).reshape(32, 32))\n",
    "mha.dense_final.params[\"w\"] = weight\n",
    "bias = rmg.DynamicVariable(np.arange(32).reshape(1,32))\n",
    "mha.dense_final.params[\"b\"] = bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.3683779e+10 8.3852050e+10 8.4020322e+10 ... 8.8563679e+10\n",
      "  8.8731943e+10 8.8900215e+10]\n",
      " [8.3683779e+10 8.3852050e+10 8.4020322e+10 ... 8.8563679e+10\n",
      "  8.8731943e+10 8.8900215e+10]\n",
      " [8.3683779e+10 8.3852050e+10 8.4020322e+10 ... 8.8563679e+10\n",
      "  8.8731943e+10 8.8900215e+10]\n",
      " ...\n",
      " [1.6679390e+11 1.6712928e+11 1.6746468e+11 ... 1.7652020e+11\n",
      "  1.7685558e+11 1.7719096e+11]\n",
      " [1.6679390e+11 1.6712928e+11 1.6746468e+11 ... 1.7652020e+11\n",
      "  1.7685558e+11 1.7719096e+11]\n",
      " [1.6679390e+11 1.6712928e+11 1.6746468e+11 ... 1.7652020e+11\n",
      "  1.7685558e+11 1.7719096e+11]]\n"
     ]
    }
   ],
   "source": [
    "result = mha(q, k, v, mask)\n",
    "print(result.numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA_Cell(rmg.core.GraphFactory):\n",
    "    def prepare(self, d_model, num_heads, dropout_rate):\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.dropout = rmg.Dropout(dropout_rate)\n",
    "        self.layernorm = rmg.LayerNormalize()\n",
    "        \n",
    "    def connect(self, q, k, v, mask, inference):\n",
    "        self.dropout.set_inference(inference)\n",
    "        output = self.mha(q, k, v, mask)  # (B*SQ, D_MODEL)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layernorm(q + output)\n",
    "        return output\n",
    "\n",
    "    \n",
    "class FFN_Cell(rmg.core.GraphFactory):\n",
    "    def prepare(self, d_model, dff, dropout_rate):\n",
    "        self.sequential = rmg.Sequential([\n",
    "            rmg.Dense(dff, activation='relu'), # (B*SQ, DFF)\n",
    "            rmg.Dense(d_model)  # (B*SQ, D_MODEL)\n",
    "        ])\n",
    "        self.dropout = rmg.Dropout(dropout_rate)\n",
    "        self.layernorm = rmg.LayerNormalize()\n",
    "        \n",
    "    def connect(self, x, inference):\n",
    "        self.dropout.set_inference(inference)\n",
    "        output = self.sequential(x)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layernorm(x + output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(rmg.core.GraphFactory):\n",
    "    def prepare(self, d_model, num_heads, dff, dropout_rate):\n",
    "        self.mha_cell = MHA_Cell(d_model, num_heads, dropout_rate)\n",
    "        self.ffn_cell = FFN_Cell(d_model, dff, dropout_rate)\n",
    "    \n",
    "    def connect(self, x, mask, inference):\n",
    "        print(\"    ENCODER MHA\")\n",
    "        output = self.mha_cell(x, x, x, mask, inference)\n",
    "        print(\"    ENCODER FFN\")\n",
    "        output = self.ffn_cell(output, inference)\n",
    "        return output\n",
    "\n",
    "class DecoderLayer(rmg.core.GraphFactory):\n",
    "    def prepare(self, d_model, num_heads, dff, dropout_rate):\n",
    "        self.mha_cell_1 = MHA_Cell(d_model, num_heads, dropout_rate)\n",
    "        self.mha_cell_2 = MHA_Cell(d_model, num_heads, dropout_rate)\n",
    "        self.ffn_cell = FFN_Cell(d_model, dff, dropout_rate)\n",
    "        \n",
    "    def connect(self, x, encoder_output, input_padding_mask, look_ahead_mask, inference):\n",
    "        print(\"    DECODER MHA 1\")\n",
    "        output = self.mha_cell_1(x, x, x, look_ahead_mask, inference)\n",
    "        print(\"    DECODER MHA 2\")\n",
    "        output = self.mha_cell_2(output, encoder_output, encoder_output, input_padding_mask, inference)      \n",
    "        print(\"    DECODER FFN\")\n",
    "        output = self.ffn_cell(output, inference)   \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(rmg.core.GraphFactory):\n",
    "    def prepare(self, num_layers, d_model, num_heads, dff, dropout_rate):\n",
    "        self.dropout = rmg.Dropout(dropout_rate)\n",
    "        self.encoder_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
    "        \n",
    "    def connect(self, x, mask, inference):\n",
    "        self.dropout.set_inference(inference)\n",
    "        print(\"  ENCODER: \")\n",
    "        x = self.dropout(x)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, mask, inference)\n",
    "        print(\"  ENCODER FINISHED!\")\n",
    "        return x  # (B*S, D_MODEL)\n",
    "    \n",
    "class Decoder(rmg.core.GraphFactory):\n",
    "    def prepare(self, num_layers, d_model, num_heads, dff, dropout_rate):\n",
    "        self.dropout = rmg.Dropout(dropout_rate)\n",
    "        self.decoder_layers = [DecoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
    "        \n",
    "    def connect(self, x, encoder_output, input_padding_mask, look_ahead_mask, inference):\n",
    "        self.dropout.set_inference(inference)\n",
    "        print(\"  DECODER: \")\n",
    "        x = self.dropout(x)\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            x = decoder_layer(x, encoder_output, input_padding_mask, look_ahead_mask, inference)\n",
    "        print(\"  DECODER FINISHED!\")\n",
    "        return x # (B*S, D_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(rmg.core.GraphFactory):\n",
    "    def prepare(self, \n",
    "                num_layers, d_model, num_heads, dff, \n",
    "                input_vocab_size, target_vocab_size, \n",
    "                pos_enc_size, dropout_rate=0.1):\n",
    "        self.d_model = d_model\n",
    "        self.sqrt_d_model = np.sqrt(d_model)\n",
    "        self.pos_enc_size = pos_enc_size\n",
    "        self.input_embedding = rmg.Embedding(input_vocab_size, d_model)\n",
    "        self.target_embedding = rmg.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_enc, self.pos_enc_tiled = None, None\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, dropout_rate)  \n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, dropout_rate)\n",
    "        self.linear = rmg.Dense(target_vocab_size)\n",
    "        self.softmax = rmg.Softmax()\n",
    "    \n",
    "    def connect(self, \n",
    "                inputs, #(B*S, 1)\n",
    "                targets, #(B*S, 1)\n",
    "                batch_size=1,\n",
    "                inference = False):\n",
    "        print(\"TRANSFORMER FORWARD: \")\n",
    "        self.batch_size = batch_size\n",
    "        input_padding_mask, look_ahead_mask = self.create_masks(inputs, targets)\n",
    "        \n",
    "#         inputs  = self.input_embedding(inputs)   * rmg.sqrt(self.d_model) #(B*S, D_MODEL)\n",
    "#         targets = self.target_embedding(targets) * rmg.sqrt(self.d_model) #(B*S, D_MODEL)\n",
    "        inputs  = self.input_embedding(inputs)   * self.sqrt_d_model #(B*S, D_MODEL)\n",
    "        targets = self.target_embedding(targets) * self.sqrt_d_model #(B*S, D_MODEL)\n",
    "        \n",
    "        if self.pos_enc is None:\n",
    "            self.pos_enc, self.pos_enc_tiled = self.get_position_encoding_matrix(self.pos_enc_size, self.d_model) \n",
    "            #(B, D_MODEL)\n",
    "        inputs  = self.add_position_encoding(inputs)\n",
    "        targets = self.add_position_encoding(targets)\n",
    "        \n",
    "        encoder_output = self.encoder(inputs, input_padding_mask, inference)  # (B*S, D_MODEL)\n",
    "        decoder_output = self.decoder(targets, encoder_output, input_padding_mask, look_ahead_mask, inference) \n",
    "        # (B*S, D_MODEL)\n",
    "\n",
    "        output = self.linear(decoder_output)  # (B*S, target_vocab_size)\n",
    "#         output = self.softmax(output) # (B*S, target_vocab_size)\n",
    "        print(\"TRANSFORMER FINISHED!\\n\")\n",
    "        return output\n",
    "    \n",
    "    def add_position_encoding(self, x):\n",
    "        if x.shape == self.pos_enc_tiled.shape:\n",
    "            return x + self.pos_enc_tiled\n",
    "        if x.shape == self.pos_enc.shape:\n",
    "            return x + self.pos_enc\n",
    "        assert False\n",
    "    \n",
    "    def get_position_encoding_matrix(self, num_position, d_model, min_rate = 1/10000):\n",
    "        angle_rates = min_rate**(np.linspace(0, 1, d_model//2))\n",
    "        positions = np.arange(num_position) \n",
    "        angle_rads = (positions[:, np.newaxis]) * angle_rates[np.newaxis, :]\n",
    "        sines, cosines = np.sin(angle_rads), np.cos(angle_rads)\n",
    "        pos_encoding = np.stack([sines, cosines], axis=2).reshape(sines.shape[0], -1)\n",
    "        pos_encoding_tiled = np.tile(pos_encoding, (self.batch_size, 1))\n",
    "        \n",
    "        pos_encoding = rmg.DynamicVariable(pos_encoding)\n",
    "        pos_encoding_tiled = rmg.DynamicVariable(pos_encoding_tiled)\n",
    "        return pos_encoding, pos_encoding_tiled\n",
    "\n",
    "    def create_masks(self, inputs, targets):\n",
    "        def create_padding_mask(seq):\n",
    "            mapping_func = np.vectorize(lambda x: x == 0, otypes=[np.float32])\n",
    "            mask = mapping_func(np.copy(seq.numpy))\n",
    "            return mask\n",
    "\n",
    "        def create_look_ahead_mask(size):\n",
    "            ones = np.ones((size, size), dtype=np.float32)\n",
    "            look_ahead_mask = np.triu(ones, k=1)\n",
    "            return look_ahead_mask \n",
    "        MAX_SEQ_LEN = targets.shape[0].value//self.batch_size\n",
    "        \n",
    "        input_padding_mask = create_padding_mask(inputs)\n",
    "        input_padding_mask = input_padding_mask.flatten().reshape(self.batch_size, MAX_SEQ_LEN)\n",
    "        input_padding_mask = np.repeat(input_padding_mask, repeats = [MAX_SEQ_LEN]*self.batch_size, axis=0)\n",
    "        \n",
    "        target_padding_mask = create_padding_mask(targets)    \n",
    "        target_padding_mask = target_padding_mask.flatten().reshape(self.batch_size, MAX_SEQ_LEN)\n",
    "        target_padding_mask = np.repeat(target_padding_mask, repeats = [MAX_SEQ_LEN]*self.batch_size, axis=0)\n",
    "        \n",
    "        look_ahead_mask = create_look_ahead_mask(MAX_SEQ_LEN) \n",
    "        look_ahead_mask = np.tile(look_ahead_mask, (self.batch_size, 1))\n",
    "        assert look_ahead_mask.shape == target_padding_mask.shape\n",
    "        look_ahead_mask = np.maximum(look_ahead_mask, target_padding_mask)\n",
    "        input_padding_mask *= -1e9\n",
    "        look_ahead_mask *= -1e9\n",
    "        input_padding_mask = rmg.DynamicVariable(input_padding_mask)\n",
    "        look_ahead_mask = rmg.DynamicVariable(look_ahead_mask)\n",
    "        \n",
    "        return input_padding_mask, look_ahead_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not tokenizer_en:\n",
    "    with open('pickle/tokenizer_en.pickle', 'rb') as f:\n",
    "        tokenizer_en = pickle.load(f)\n",
    "if not tokenizer_pt:\n",
    "    with open('pickle/tokenizer_pt.pickle', 'rb') as f:\n",
    "        tokenizer_pt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel():\n",
    "    def __init__(self):\n",
    "        self.MAX_SEQ_LEN = 40\n",
    "        self.BATCH_SIZE = 1\n",
    "\n",
    "        self.model = Transformer(num_layers = 2,\n",
    "                                 d_model = 128,\n",
    "                                 num_heads = 4,\n",
    "                                 dff = 256, \n",
    "                                 input_vocab_size = tokenizer_pt.vocab_size + 2,\n",
    "                                 target_vocab_size = tokenizer_en.vocab_size + 2,\n",
    "                                 pos_enc_size = self.MAX_SEQ_LEN,\n",
    "                                 dropout_rate = 0.1)\n",
    "        self.loss = rmg.SoftmaxCrossEntropy()\n",
    "        self.opt = rmg.Adam(beta1=0.9, beta2=0.98, epsilon=1e-8)\n",
    "#         self.opt = rmg.Adam()\n",
    "#         self.opt = rmg.Rmsprop(lr=0.05)\n",
    "        \n",
    "    def fit(self, x, y_inp, y_tar):\n",
    "        y_tar = to_categorical(y_tar)\n",
    "        dist = rmg.SimpleGenerator(x, y_inp, y_tar).shuffle().batch(self.BATCH_SIZE*self.MAX_SEQ_LEN)\n",
    "        x_in, y_in, target = dist.get_output_graphs(num_gpus=1)\n",
    "#         print(\"MODEL INPUT SHAPE: \", x_in.shape, y_in.shape, target.shape) \n",
    "        prediction = self.model(inputs=x_in, targets=y_in, batch_size=self.BATCH_SIZE)\n",
    "#         assert prediction.shape == target.shape      \n",
    "        self.graph = self.loss(prediction, target)\n",
    "        self.graph.backward()\n",
    "        self.exe = self.graph.get_executor(mode='training', optimizer=self.opt)\n",
    "        \n",
    "    def execute(self, epochs = 1):\n",
    "        self.exe.execute(epochs=epochs)\n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def save(self, name=None):\n",
    "        if name is None:\n",
    "            name = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\").replace(\"/\",\"_\").replace(\":\", \"_\").replace(\" \", \"_\")\n",
    "        full_name = \"saved_transformer/transformer_2_\" + name + \".hdf5\"\n",
    "        return self.model.save(full_name)\n",
    "    \n",
    "    def load(self, name):\n",
    "        return self.model.load(\"saved_transformer/\" + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not tokenized_data:\n",
    "    with open('pickle2/tokenized_data.pickle', 'rb') as f:\n",
    "        tokenized_data = pickle.load(f)\n",
    "data = \"train\"\n",
    "train_X = tokenized_data[\"pt.\" + data]\n",
    "train_y_inp = tokenized_data[\"en.\" + data + \".inp\"]\n",
    "train_y_tar = tokenized_data[\"en.\" + data + \".tar\"]\n",
    "\n",
    "transformer = TransformerModel()\n",
    "writer = SummaryWriter(logdir='tensorboardX_data/transformer')\n",
    "loss_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(threshold=100000)\n",
    "# transformer.fit(train_X, train_y_inp, train_y_tar, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "transformer.fit(train_X, train_y_inp, train_y_tar)\n",
    "\n",
    "for epoch in range (0, num_epochs):\n",
    "    print(\"EPOCH : \", epoch)\n",
    "    gc.collect()\n",
    "    transformer.execute()\n",
    "    transformer.save()\n",
    "    loss_val = transformer.graph.output.as_ndarray()[0]\n",
    "    print(\"LOSS: \", loss_val)\n",
    "    loss_list.append(loss_val)\n",
    "    writer.add_scalar('Transformer loss w/ more layers', loss_val, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer.fit(train_X, train_y_inp, train_y_tar, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer2 = TransformerModel()\n",
    "# transformer2.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir tensorboardX_data/transformer2 --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token_en = [tokenizer_en.vocab_size]\n",
    "end_token_en = [tokenizer_en.vocab_size + 1]\n",
    "start_token_pt = [tokenizer_pt.vocab_size]\n",
    "end_token_pt = [tokenizer_pt.vocab_size + 1]\n",
    "\n",
    "MAX_SEQ_LEN = 40\n",
    "\n",
    "def translate(model, sentence):\n",
    "    encoder_input = [start_token_pt] + [[i] for i in tokenizer_pt.encode(sentence)] + [end_token_pt]\n",
    "    encoder_input = encoder_input + [[0]] * (MAX_SEQ_LEN - len(encoder_input))\n",
    "    encoder_input = rmg.DynamicVariable(np.asarray(encoder_input))\n",
    "    decoder_input = [start_token_en]\n",
    "\n",
    "    for i in range(MAX_SEQ_LEN):\n",
    "#         print(decoder_input)\n",
    "        output = decoder_input + [[0]] * (MAX_SEQ_LEN - len(decoder_input))\n",
    "        output = rmg.DynamicVariable(np.asarray(output))\n",
    "#         print(\"TRANSLATE INPUT SHAPE \", encoder_input.shape, output.shape)\n",
    "        prediction = model(inputs=encoder_input, targets=output, inference=True)\n",
    "#         return prediction\n",
    "#             print(\"PREDICTION: \", prediction)\n",
    "        last_word = prediction[len(decoder_input)-1:]\n",
    "        predicted_id = rmg.argmax(last_word, axis=1).numpy[0]\n",
    "        if predicted_id == end_token_en:\n",
    "            break\n",
    "        decoder_input.append([predicted_id])\n",
    "\n",
    "#         clear_output()\n",
    "    print(decoder_input)\n",
    "    token_list = [int(token[0]) for token in decoder_input if token[0] < tokenizer_en.vocab_size]\n",
    "    translated_sentence = tokenizer_en.decode(token_list)\n",
    "    return translated_sentence    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = \"este Ã© um problema que temos que resolver.\"\n",
    "translate(transformer.get_model(), seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
